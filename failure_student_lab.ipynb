{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classical ML Failure Modes — Student Lab (Titanic)\n",
        "\n",
        "This lab focuses on failure modes: leakage, spurious correlations, and bad validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Load Titanic (Kaggle) with fallback\n",
        "\n",
        "Expected path: `data/titanic/train.csv`\n",
        "\n",
        "If missing, we generate a tiny synthetic dataset so notebook still runs." 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_titanic_or_synthetic():\n",
        "    path = os.path.join(os.getcwd(), 'data', 'titanic', 'train.csv')\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        return 'kaggle', df\n",
        "\n",
        "    # synthetic fallback (schema resembles Titanic)\n",
        "    df = pd.DataFrame({\n",
        "        'Survived': [0,1,1,0,1,0,0,1],\n",
        "        'Pclass': [3,1,3,3,2,3,2,1],\n",
        "        'Sex': ['male','female','female','male','female','male','male','female'],\n",
        "        'Age': [22, 38, 26, 35, 28, 2, 54, 19],\n",
        "        'SibSp': [1,1,0,1,0,3,0,0],\n",
        "        'Parch': [0,0,0,0,0,1,0,0],\n",
        "        'Fare': [7.25, 71.3, 7.92, 53.1, 13.0, 21.1, 51.9, 30.0],\n",
        "        'Embarked': ['S','C','S','S','S','S','S','C'],\n",
        "    })\n",
        "    return 'synthetic', df\n",
        "\n",
        "mode, df = load_titanic_or_synthetic()\n",
        "print('mode:', mode, 'rows:', len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Baseline + Proper Validation\n",
        "\n",
        "### Task 1.1: Minimal baseline features\n",
        "\n",
        "Use only: Pclass, Sex, Age, Fare (simple).\n",
        "\n",
        "# TODO:\n",
        "- Create X/y\n",
        "- Handle missing Age/Fare\n",
        "- One-hot encode Sex\n",
        "\n",
        "**Checkpoint:** Why do we start with a minimal baseline?" 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "y = df['Survived'].astype(int).values\n",
        "X = df[['Pclass','Sex','Age','Fare']].copy()\n",
        "\n",
        "# impute\n",
        "X['Age'] = X['Age'].fillna(X['Age'].median())\n",
        "X['Fare'] = X['Fare'].fillna(X['Fare'].median())\n",
        "\n",
        "# one-hot\n",
        "X = pd.get_dummies(X, columns=['Sex'], drop_first=True)\n",
        "\n",
        "Xtr, Xva, ytr, yva = train_test_split(X.values, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "clf = LogisticRegression(max_iter=2000)\n",
        "clf.fit(Xtr, ytr)\n",
        "pred = clf.predict(Xva)\n",
        "proba = clf.predict_proba(Xva)[:,1]\n",
        "print('acc', accuracy_score(yva, pred))\n",
        "print('auc', roc_auc_score(yva, proba))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Leakage\n",
        "\n",
        "### Task 2.1: Create a leaky feature\n",
        "\n",
        "Intentionally create a feature that encodes the label (e.g., `leak = Survived`).\n",
        "Train again and observe metric inflation.\n",
        "\n",
        "**Checkpoint:** How can you detect leakage quickly in an interview?" 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_leak = X.copy()\n",
        "X_leak['leak'] = df['Survived'].values  # leaky on purpose\n",
        "\n",
        "Xtr, Xva, ytr, yva = train_test_split(X_leak.values, y, test_size=0.3, random_state=0, stratify=y)\n",
        "clf = LogisticRegression(max_iter=2000)\n",
        "clf.fit(Xtr, ytr)\n",
        "print('acc_with_leak', accuracy_score(yva, clf.predict(Xva)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Spurious correlations + slice analysis\n",
        "\n",
        "### Task 3.1: Evaluate slices\n",
        "\n",
        "Compute accuracy by groups (Sex, Pclass).\n",
        "\n",
        "# TODO:\n",
        "- Make predictions on validation\n",
        "- Report metrics by slice\n",
        "\n",
        "**Interview Angle:** Why can overall accuracy hide severe subgroup failures?" 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refit baseline quickly (without leak)\n",
        "Xbase = df[['Pclass','Sex','Age','Fare']].copy()\n",
        "Xbase['Age'] = Xbase['Age'].fillna(Xbase['Age'].median())\n",
        "Xbase['Fare'] = Xbase['Fare'].fillna(Xbase['Fare'].median())\n",
        "Xbase = pd.get_dummies(Xbase, columns=['Sex'], drop_first=True)\n",
        "\n",
        "idx = np.arange(len(df))\n",
        "tr_idx, va_idx = train_test_split(idx, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "clf = LogisticRegression(max_iter=2000)\n",
        "clf.fit(Xbase.iloc[tr_idx].values, y[tr_idx])\n",
        "pred = clf.predict(Xbase.iloc[va_idx].values)\n",
        "\n",
        "va_df = df.iloc[va_idx].copy()\n",
        "va_df['pred'] = pred\n",
        "va_df['correct'] = (va_df['pred'].values == va_df['Survived'].values).astype(int)\n",
        "\n",
        "print('overall_acc', va_df['correct'].mean())\n",
        "print('acc_by_sex')\n",
        "print(va_df.groupby('Sex')['correct'].mean())\n",
        "print('acc_by_pclass')\n",
        "print(va_df.groupby('Pclass')['correct'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Dataset shift (toy simulation)\n",
        "\n",
        "### Task 4.1: Simulate a shift in Fare distribution\n",
        "\n",
        "Create a shifted validation set by multiplying Fare and see how performance changes.\n",
        "\n",
        "**Checkpoint:** How would you monitor drift in production?" 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_shift = Xbase.copy()\n",
        "# toy shift\n",
        "if 'Fare' in X_shift.columns:\n",
        "    X_shift['Fare'] = X_shift['Fare'] * 3.0\n",
        "\n",
        "pred_shift = clf.predict(X_shift.iloc[va_idx].values)\n",
        "acc_shift = accuracy_score(y[va_idx], pred_shift)\n",
        "print('acc_original', accuracy_score(y[va_idx], pred))\n",
        "print('acc_shifted', acc_shift)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- Baseline trained + evaluated\n",
        "- Leakage demo shown\n",
        "- Slice metrics reported\n",
        "- Shift simulation discussed\n"
      ]
    }
  ]
}
